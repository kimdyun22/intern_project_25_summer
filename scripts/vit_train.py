import os
import torch
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from timm import create_model
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from sklearn.metrics import classification_report
from tqdm import tqdm

# ê²½ë¡œ ì„¤ì • (ìœˆë„ìš° -> ë¦¬ëˆ…ìŠ¤ ì»¨í…Œì´ë„ˆ ê²½ë¡œë¡œ ìë™ ë³€í™˜)
DATA_DIR = "/workspace/dataset_split"
BATCH_SIZE = 32
NUM_EPOCHS = 20
IMAGE_SIZE = 224
NUM_CLASSES = 2
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ë°ì´í„° ì „ì²˜ë¦¬
transform = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)
])

# ë°ì´í„°ì…‹ ë¡œë“œ
train_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, 'train'), transform=transform)
val_dataset   = datasets.ImageFolder(os.path.join(DATA_DIR, 'val'), transform=transform)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model = create_model('vit_base_patch16_224', pretrained=True)
model.head = nn.Linear(model.head.in_features, NUM_CLASSES)
model.to(DEVICE)

# ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬
criterion = nn.CrossEntropyLoss()
optimizer = AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)
scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)

# í•™ìŠµ í•¨ìˆ˜
def train_one_epoch():
    model.train()
    running_loss = 0.0
    correct = 0

    for inputs, labels in tqdm(train_loader, desc="Train"):
        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * inputs.size(0)
        preds = outputs.argmax(dim=1)
        correct += (preds == labels).sum().item()

    epoch_loss = running_loss / len(train_dataset)
    epoch_acc = correct / len(train_dataset)
    return epoch_loss, epoch_acc

# ê²€ì¦ í•¨ìˆ˜
def validate():
    model.eval()
    running_loss = 0.0
    correct = 0
    all_labels, all_preds = [], []

    with torch.no_grad():
        for inputs, labels in tqdm(val_loader, desc="Val  "):
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item() * inputs.size(0)
            preds = outputs.argmax(dim=1)
            correct += (preds == labels).sum().item()

            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())

    epoch_loss = running_loss / len(val_dataset)
    epoch_acc = correct / len(val_dataset)

    print("\nğŸ“Š Classification Report:\n")
    print(classification_report(all_labels, all_preds, target_names=val_dataset.classes))
    return epoch_loss, epoch_acc

# í•™ìŠµ ë£¨í”„
for epoch in range(NUM_EPOCHS):
    print(f"\n===== Epoch {epoch+1}/{NUM_EPOCHS} =====")

    train_loss, train_acc = train_one_epoch()
    val_loss, val_acc = validate()
    scheduler.step()

    print(f"âœ… Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f}")
    print(f"ğŸ”  Val  Loss: {val_loss:.4f} | Acc: {val_acc:.4f}")

# ëª¨ë¸ ì €ì¥
torch.save(model.state_dict(), "vit_brake_classifier.pth")
print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: vit_brake_classifier.pth")
